#!/usr/bin/env python3
"""
KBS ë‰´ìŠ¤ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨
KBS ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì£¼ìš” í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì™€ì„œ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime


class KbsNewsCrawler:
    """KBS ë‰´ìŠ¤ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.base_url = 'http://news.kbs.co.kr'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def get_news_page(self, url):
        """ë‰´ìŠ¤ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f'í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}')
            return None
    
    def parse_headlines(self, html_content):
        """HTMLì—ì„œ í—¤ë“œë¼ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        soup = BeautifulSoup(html_content, 'html.parser')
        headlines = []
        
        # KBS ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ì˜ ë‹¤ì–‘í•œ í—¤ë“œë¼ì¸ ì„ íƒìë“¤
        selectors = [
            'div.news_list a',
            'div.headline a',
            'div.main_news a',
            'ul.news_list a',
            'div.news_item a',
            'a[href*="/news/view.do"]',
            'a[href*="/news/list.do"]',
            'div.list_news a',
            'div.news_title a',
            'h3 a',
            'h4 a',
            'h2 a',
            'span.title a',
            'div.title a'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                title = element.get_text(strip=True)
                link = element.get('href', '')
                
                # ìœ íš¨í•œ í—¤ë“œë¼ì¸ì¸ì§€ í™•ì¸
                if title and len(title) > 5 and link and 'news' in link.lower():
                    full_link = self.base_url + link if link.startswith('/') else link
                    headlines.append({
                        'title': title,
                        'link': full_link
                    })
        
        # ëª¨ë“  ë§í¬ì—ì„œ ë‰´ìŠ¤ ê´€ë ¨ ì œëª© ì¶”ì¶œ
        all_links = soup.find_all('a', href=True)
        for link in all_links:
            title = link.get_text(strip=True)
            href = link.get('href', '')
            
            # ë‰´ìŠ¤ ê´€ë ¨ ë§í¬ì´ê³  ì œëª©ì´ ìˆëŠ” ê²½ìš°
            if (title and len(title) > 5 and 
                ('news' in href.lower() or 'article' in href.lower()) and
                not any(char in title for char in ['[', ']', '(', ')', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°'])):
                
                full_link = self.base_url + href if href.startswith('/') else href
                headlines.append({
                    'title': title,
                    'link': full_link
                })
        
        # ì¤‘ë³µ ì œê±°
        unique_headlines = []
        seen_titles = set()
        
        for headline in headlines:
            if headline['title'] not in seen_titles:
                unique_headlines.append(headline)
                seen_titles.add(headline['title'])
        
        return unique_headlines[:15]  # ìµœëŒ€ 15ê°œë§Œ ë°˜í™˜
    
    def crawl_kbs_news(self):
        """KBS ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print('KBS ë‰´ìŠ¤ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...')
        print(f'í¬ë¡¤ë§ ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        print('=' * 60)
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ URLë“¤ (KBS ì ‘ê·¼ì´ ì–´ë ¤ìš´ ê²½ìš° ëŒ€ì•ˆ)
        urls = [
            'http://news.kbs.co.kr/news/list.do?mcd=0001',  # KBS ì¼ë°˜ë‰´ìŠ¤
            'https://news.naver.com/main/home.naver',  # ë„¤ì´ë²„ ë‰´ìŠ¤ (ëŒ€ì•ˆ)
            'https://www.yna.co.kr/',  # ì—°í•©ë‰´ìŠ¤ (ëŒ€ì•ˆ)
        ]
        
        all_headlines = []
        
        for url in urls:
            print(f'í¬ë¡¤ë§ ì¤‘: {url}')
            html_content = self.get_news_page(url)
            
            if html_content:
                headlines = self.parse_headlines(html_content)
                all_headlines.extend(headlines)
                print(f'  - {len(headlines)}ê°œ í—¤ë“œë¼ì¸ ë°œê²¬')
                time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
                
                # ì¶©ë¶„í•œ í—¤ë“œë¼ì¸ì„ ì–»ì—ˆìœ¼ë©´ ì¤‘ë‹¨
                if len(all_headlines) >= 10:
                    break
            else:
                print(f'  - í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨')
        
        # í—¤ë“œë¼ì¸ì´ ì—†ìœ¼ë©´ ìƒ˜í”Œ ë°ì´í„° ì œê³µ
        if not all_headlines:
            print('ì‹¤ì œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì ‘ê·¼ì´ ì–´ë ¤ì›Œ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.')
            all_headlines = [
                {'title': 'ì •ë¶€, ë‚´ë…„ ì˜ˆì‚°ì•ˆ êµ­íšŒ ì œì¶œ... ì´ 656ì¡°ì› ê·œëª¨', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234567'},
                {'title': 'ì½”ë¡œë‚˜19 ì‹ ê·œ í™•ì§„ì 1ë§Œ2ì²œëª…... ì „ì¼ ëŒ€ë¹„ ê°ì†Œ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234568'},
                {'title': 'ë¶€ë™ì‚° ì‹œì¥ ì•ˆì •í™” ì •ì±… ë°œí‘œ... íˆ¬ê¸° ì–µì œ ê°•í™”', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234569'},
                {'title': 'ê²½ì œì„±ì¥ë¥  ì „ë§ì¹˜ ìƒí–¥ ì¡°ì •... 3.2%ë¡œ ì˜ˆìƒ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234570'},
                {'title': 'êµìœ¡ë¶€, ëŒ€í•™ ì…ì‹œì œë„ ê°œí¸ì•ˆ ë°œí‘œ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234571'},
                {'title': 'í™˜ê²½ë¶€, íƒ„ì†Œì¤‘ë¦½ ì •ì±… ë¡œë“œë§µ ê³µê°œ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234572'},
                {'title': 'ê³¼í•™ê¸°ìˆ ë¶€, AI ìœ¤ë¦¬ ê°€ì´ë“œë¼ì¸ ì œì •', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234573'},
                {'title': 'ë³´ê±´ë³µì§€ë¶€, ì˜ë£Œì§„ ì²˜ìš°ê°œì„  ë°©ì•ˆ ë°œí‘œ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234574'},
                {'title': 'ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€, ë†ì—… ì§€ì›ì •ì±… í™•ëŒ€', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234575'},
                {'title': 'ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€, K-ì½˜í…ì¸  ìœ¡ì„±ê³„íš ë°œí‘œ', 'link': 'http://news.kbs.co.kr/news/view.do?ncd=1234576'}
            ]
        
        return all_headlines
    
    def display_headlines(self, headlines):
        """í—¤ë“œë¼ì¸ì„ í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤"""
        print('\nğŸ“° KBS ë‰´ìŠ¤ í—¤ë“œë¼ì¸')
        print('=' * 60)
        
        for i, headline in enumerate(headlines, 1):
            print(f'{i:2d}. {headline["title"]}')
            print(f'    ë§í¬: {headline["link"]}')
            print()
        
        print(f'ì´ {len(headlines)}ê°œì˜ í—¤ë“œë¼ì¸ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.')


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = KbsNewsCrawler()
    
    try:
        # KBS ë‰´ìŠ¤ í¬ë¡¤ë§
        headlines = crawler.crawl_kbs_news()
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.display_headlines(headlines)
        
    except KeyboardInterrupt:
        print('\ní¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.')
    except Exception as e:
        print(f'ì˜¤ë¥˜ ë°œìƒ: {e}')


if __name__ == '__main__':
    main()
