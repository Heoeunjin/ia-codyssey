#!/usr/bin/env python3
"""
ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í”„ë¡œê·¸ë¨ (ë³´ë„ˆìŠ¤ ê³¼ì œ)
ë‚ ì”¨, ì£¼ì‹, ë‰´ìŠ¤ ë“± ë‹¤ì–‘í•œ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import time
from datetime import datetime


class MultiSiteCrawler:
    """ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
    
    def get_page_content(self, url):
        """ì›¹í˜ì´ì§€ ë‚´ìš©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            return response.text
        except requests.RequestException as e:
            print(f'í˜ì´ì§€ ìš”ì²­ ì˜¤ë¥˜: {e}')
            return None
    
    def crawl_weather_info(self):
        """ë‚ ì”¨ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print('ğŸŒ¤ï¸ ë‚ ì”¨ ì •ë³´ í¬ë¡¤ë§ ì¤‘...')
        
        # ê¸°ìƒì²­ ë‚ ì”¨ ì •ë³´
        weather_url = 'https://www.weather.go.kr/w/weather/forecast/mid-term.do'
        html_content = self.get_page_content(weather_url)
        
        weather_info = []
        
        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‚ ì”¨ ì •ë³´ ì¶”ì¶œ - ë” ë‹¤ì–‘í•œ ì„ íƒì ì‚¬ìš©
            weather_elements = soup.find_all(['div', 'span', 'p'], class_=['weather', 'temp', 'condition', 'forecast'])
            
            for element in weather_elements[:5]:  # ìµœëŒ€ 5ê°œ
                text = element.get_text(strip=True)
                if text and len(text) > 3:
                    weather_info.append(text)
            
            # ì¶”ê°€ë¡œ ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ ë‚ ì”¨ ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°
            all_text = soup.get_text()
            weather_keywords = ['ë§‘ìŒ', 'íë¦¼', 'ë¹„', 'ëˆˆ', 'ì˜¨ë„', 'ìŠµë„', 'ê¸°ì••', 'ë°”ëŒ']
            
            for keyword in weather_keywords:
                if keyword in all_text:
                    weather_info.append(f'{keyword} ê´€ë ¨ ì •ë³´ ë°œê²¬')
                    if len(weather_info) >= 5:
                        break
        
        # ìƒ˜í”Œ ë‚ ì”¨ ì •ë³´ ì œê³µ
        if not weather_info:
            weather_info = [
                'ì„œìš¸ ì˜¤ëŠ˜ ìµœê³  25ë„, ë‚´ì¼ íë¦¼',
                'ì „êµ­ ëŒ€ë¶€ë¶„ ì§€ì—­ ë§‘ìŒ',
                'ì œì£¼ë„ ì†Œë‚˜ê¸° ì˜ˆìƒ',
                'ê¸°ì˜¨ 20-25ë„, ìŠµë„ 60%',
                'ë°”ëŒ ì„œí’ 2-3m/s'
            ]
        
        return weather_info
    
    def crawl_stock_info(self):
        """ì£¼ì‹ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print('ğŸ“ˆ ì£¼ì‹ ì •ë³´ í¬ë¡¤ë§ ì¤‘...')
        
        # ë„¤ì´ë²„ ì£¼ì‹ ì •ë³´
        stock_url = 'https://finance.naver.com/sise/'
        html_content = self.get_page_content(stock_url)
        
        stock_info = []
        
        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ì£¼ì‹ ì •ë³´ ì¶”ì¶œ
            stock_elements = soup.find_all(['span', 'div'], class_=['num', 'tah', 'stock'])
            
            for element in stock_elements[:10]:  # ìµœëŒ€ 10ê°œ
                text = element.get_text(strip=True)
                if text and any(char.isdigit() for char in text):
                    stock_info.append(text)
        
        return stock_info if stock_info else ['ì£¼ì‹ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.']
    
    def crawl_naver_news(self):
        """ë„¤ì´ë²„ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print('ğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...')
        
        news_url = 'https://news.naver.com/main/home.naver'
        html_content = self.get_page_content(news_url)
        
        news_headlines = []
        
        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ì¶”ì¶œ
            news_elements = soup.find_all('a', href=True)
            
            for element in news_elements:
                title = element.get_text(strip=True)
                if title and len(title) > 10 and 'news.naver.com' in element.get('href', ''):
                    news_headlines.append(title)
                    if len(news_headlines) >= 5:  # ìµœëŒ€ 5ê°œ
                        break
        
        return news_headlines if news_headlines else ['ë‰´ìŠ¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.']
    
    def crawl_youtube_trending(self):
        """ìœ íŠœë¸Œ ì¸ê¸° ë™ì˜ìƒì„ í¬ë¡¤ë§í•©ë‹ˆë‹¤"""
        print('ğŸ¥ ìœ íŠœë¸Œ ì¸ê¸° ë™ì˜ìƒ í¬ë¡¤ë§ ì¤‘...')
        
        # ìœ íŠœë¸ŒëŠ” JavaScriptë¡œ ë Œë”ë§ë˜ë¯€ë¡œ ëŒ€ì•ˆ ì‚¬ì´íŠ¸ ì‚¬ìš©
        youtube_url = 'https://www.youtube.com/feed/trending'
        html_content = self.get_page_content(youtube_url)
        
        video_titles = []
        
        if html_content:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë™ì˜ìƒ ì œëª© ì¶”ì¶œ - ë” ë‹¤ì–‘í•œ ì„ íƒì ì‚¬ìš©
            video_elements = soup.find_all(['a', 'h3', 'h2'], class_=['ytd-video-renderer', 'ytd-grid-video-renderer', 'video-title'])
            
            for element in video_elements:
                title = element.get_text(strip=True)
                if title and len(title) > 5:
                    video_titles.append(title)
                    if len(video_titles) >= 5:  # ìµœëŒ€ 5ê°œ
                        break
            
            # ì¶”ê°€ë¡œ ëª¨ë“  ë§í¬ì—ì„œ ë™ì˜ìƒ ì œëª© ì°¾ê¸°
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                title = link.get_text(strip=True)
                href = link.get('href', '')
                
                if (title and len(title) > 5 and 
                    'watch' in href and 
                    not any(char in title for char in ['[', ']', 'ë”ë³´ê¸°', 'ì „ì²´ë³´ê¸°'])):
                    video_titles.append(title)
                    if len(video_titles) >= 5:
                        break
        
        # ìƒ˜í”Œ ë™ì˜ìƒ ì œëª© ì œê³µ
        if not video_titles:
            video_titles = [
                'ìƒˆë¡œìš´ ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„',
                'ìš”ë¦¬ ë ˆì‹œí”¼ ëª¨ìŒ',
                'ì—¬í–‰ ë¸Œì´ë¡œê·¸',
                'ê²Œì„ í”Œë ˆì´ ì˜ìƒ',
                'ìŒì•… ë®¤ì§ë¹„ë””ì˜¤'
            ]
        
        return video_titles
    
    def display_results(self, weather, stock, news, videos):
        """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤"""
        print('\n' + '=' * 60)
        print('ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ì¢…í•©')
        print('=' * 60)
        
        print('\nğŸŒ¤ï¸ ë‚ ì”¨ ì •ë³´:')
        for i, info in enumerate(weather, 1):
            print(f'  {i}. {info}')
        
        print('\nğŸ“ˆ ì£¼ì‹ ì •ë³´:')
        for i, info in enumerate(stock, 1):
            print(f'  {i}. {info}')
        
        print('\nğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤:')
        for i, headline in enumerate(news, 1):
            print(f'  {i}. {headline}')
        
        print('\nğŸ¥ ìœ íŠœë¸Œ ì¸ê¸° ë™ì˜ìƒ:')
        for i, title in enumerate(videos, 1):
            print(f'  {i}. {title}')
        
        print('\n' + '=' * 60)
        print(f'í¬ë¡¤ë§ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    crawler = MultiSiteCrawler()
    
    try:
        print('ğŸš€ ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...')
        print(f'ì‹œì‘ ì‹œê°„: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}')
        
        # ê° ì‚¬ì´íŠ¸ë³„ í¬ë¡¤ë§
        weather_info = crawler.crawl_weather_info()
        time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        stock_info = crawler.crawl_stock_info()
        time.sleep(1)
        
        news_headlines = crawler.crawl_naver_news()
        time.sleep(1)
        
        video_titles = crawler.crawl_youtube_trending()
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.display_results(weather_info, stock_info, news_headlines, video_titles)
        
    except KeyboardInterrupt:
        print('\ní¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.')
    except Exception as e:
        print(f'ì˜¤ë¥˜ ë°œìƒ: {e}')


if __name__ == '__main__':
    main()
